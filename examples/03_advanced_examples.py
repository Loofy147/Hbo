#!/usr/bin/env python3
"""
Ø£Ù…Ø«Ù„Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ØªØ®ØµÙŠØµ Ù†Ø¸Ø§Ù… HPO ÙˆØ¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
Advanced Examples for HPO System Customization and Extended Features

ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (hpo_system.py) Ù„ÙŠØ¹Ù…Ù„
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import time
import warnings
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, List, Any, Optional, Callable
import logging

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
# HPOSystem is an alias for the main Study class, used for consistency with the original script's terminology.
from hpo import Study as HPOSystem
from hpo import SearchSpace

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©
try:
    import optuna
    HAS_OPTUNA = True
except ImportError:
    HAS_OPTUNA = False

try:
    from sklearn.model_selection import cross_val_score, StratifiedKFold, TimeSeriesSplit
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.datasets import make_classification, load_digits, fetch_california_housing
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False

warnings.filterwarnings('ignore')

# ======================================================================
# 1. ØªØ®ØµÙŠØµ Ù„Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© - Domain-Specific Optimizers
# ======================================================================

class FinancialModelOptimizer:
    """Ù…Ø­Ø³Ù† Ù…Ø®ØµØµ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ©"""

    def __init__(self, price_data, returns_data, risk_free_rate=0.02):
        """
        price_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
        returns_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯
        risk_free_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø®Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        """
        self.price_data = np.array(price_data)
        self.returns_data = np.array(returns_data)
        self.risk_free_rate = risk_free_rate

        print(f"ğŸ“ˆ Ù…Ø­Ø³Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ©")
        print(f"   Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±: {len(price_data)} Ù†Ù‚Ø·Ø©")
        print(f"   Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø®Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ù…Ø®Ø§Ø·Ø±: {risk_free_rate:.2%}")

    def optimize_portfolio_weights(self, n_assets=5, n_trials=50):
        """ØªØ­Ø³ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙØ¸Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨"""

        print(f"ğŸ¯ ØªØ­Ø³ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙØ¸Ø© ({n_assets} Ø£ØµÙˆÙ„)")

        def portfolio_objective(trial):
            # ØªÙˆÙ„ÙŠØ¯ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙØ¸Ø© (ÙŠØ¬Ø¨ Ø£Ù† ØªØµÙ„ Ù„Ù€ 1)
            weights = []
            remaining_weight = 1.0

            for i in range(n_assets - 1):
                # ÙƒÙ„ ÙˆØ²Ù† ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ù† 0 Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
                max_weight = min(0.5, remaining_weight)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 50% Ù„ÙƒÙ„ Ø£ØµÙ„
                weight = trial.suggest_float(f'weight_{i}', 0.0, max_weight)
                weights.append(weight)
                remaining_weight -= weight

            # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø®ÙŠØ± Ù‡Ùˆ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
            weights.append(max(0.0, remaining_weight))
            weights = np.array(weights)

            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù„ØªØµÙ„ Ù„Ù€ 1
            if weights.sum() > 0:
                weights = weights / weights.sum()
            else:
                weights = np.ones(n_assets) / n_assets

            # Ø­Ø³Ø§Ø¨ Ø¹Ø§Ø¦Ø¯ ÙˆÙ…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø­ÙØ¸Ø©
            # We select columns (assets) and calculate stats along rows (time)
            asset_returns = self.returns_data[:, :n_assets]
            mean_returns = np.mean(asset_returns, axis=0)
            asset_variances = np.var(asset_returns, axis=0)

            portfolio_return = np.dot(weights, mean_returns)
            portfolio_volatility = np.sqrt(np.dot(weights**2, asset_variances))

            # Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨
            if portfolio_volatility > 0:
                sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_volatility
            else:
                sharpe_ratio = -10  # Ø¹Ù‚ÙˆØ¨Ø© Ù„Ù„ØªÙ‚Ù„Ø¨ Ø§Ù„ØµÙØ±ÙŠ

            return sharpe_ratio

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        for i in range(n_assets - 1):
            search_space.add_uniform(f'weight_{i}', 0.0, 0.5)

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=portfolio_objective,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='portfolio_optimization',
            verbose=True
        )

        best_trial = hpo.optimize()

        if best_trial:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø«Ù„Ù‰
            weights = []
            remaining_weight = 1.0

            for i in range(n_assets - 1):
                weight = best_trial.params[f'weight_{i}']
                weights.append(weight)
                remaining_weight -= weight

            weights.append(max(0.0, remaining_weight))
            weights = np.array(weights)
            weights = weights / weights.sum()  # ØªØ·Ø¨ÙŠØ¹

            print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù…Ø­ÙØ¸Ø©:")
            print(f"   Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨: {best_trial.value:.4f}")
            print(f"   Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙØ¸Ø©:")
            for i, weight in enumerate(weights):
                print(f"     Ø£ØµÙ„ {i+1}: {weight:.3f} ({weight*100:.1f}%)")

            return weights, best_trial.value, hpo

        return None, 0, hpo

    def optimize_trading_strategy(self, n_trials=30):
        """ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ¯Ø§ÙˆÙ„"""

        print("ğŸ“Š ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ¯Ø§ÙˆÙ„")

        def trading_objective(trial):
            # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
            short_window = trial.suggest_int('short_window', 5, 20)
            long_window = trial.suggest_int('long_window', 20, 100)
            threshold = trial.suggest_float('threshold', 0.01, 0.1)
            stop_loss = trial.suggest_float('stop_loss', 0.02, 0.1)

            # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
            if short_window >= long_window:
                return -1  # Ø¹Ù‚ÙˆØ¨Ø©

            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
            prices = self.price_data
            short_ma = pd.Series(prices).rolling(short_window).mean().values
            long_ma = pd.Series(prices).rolling(long_window).mean().values

            positions = np.zeros(len(prices))
            returns = np.zeros(len(prices))

            for i in range(long_window, len(prices)):
                # Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø´Ø±Ø§Ø¡/Ø§Ù„Ø¨ÙŠØ¹
                if short_ma[i] > long_ma[i] * (1 + threshold):
                    positions[i] = 1  # Ø´Ø±Ø§Ø¡
                elif short_ma[i] < long_ma[i] * (1 - threshold):
                    positions[i] = -1  # Ø¨ÙŠØ¹
                else:
                    positions[i] = positions[i-1]  # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ÙˆØ¶Ø¹

                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ø§Ø¦Ø¯
                if i > 0:
                    price_return = (prices[i] - prices[i-1]) / prices[i-1]
                    returns[i] = positions[i-1] * price_return

                    # ØªØ·Ø¨ÙŠÙ‚ stop loss
                    if abs(returns[i]) > stop_loss:
                        positions[i] = 0  # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ÙˆØ¶Ø¹

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡
            total_return = np.sum(returns)
            volatility = np.std(returns) if np.std(returns) > 0 else 0.01
            sharpe = total_return / volatility if volatility > 0 else 0

            return sharpe

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        search_space.add_int('short_window', 5, 20)
        search_space.add_int('long_window', 20, 100)
        search_space.add_uniform('threshold', 0.01, 0.1)
        search_space.add_uniform('stop_loss', 0.02, 0.1)

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=trading_objective,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='trading_strategy',
            verbose=True
        )

        best_trial = hpo.optimize()

        if best_trial:
            print(f"\nğŸ¯ Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:")
            print(f"   Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨: {best_trial.value:.4f}")
            print(f"   Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø©: {best_trial.params['short_window']} ÙŠÙˆÙ…")
            print(f"   Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø©: {best_trial.params['long_window']} ÙŠÙˆÙ…")
            print(f"   Ø§Ù„Ø¹ØªØ¨Ø©: {best_trial.params['threshold']:.3f}")
            print(f"   ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {best_trial.params['stop_loss']:.3f}")

        return best_trial, hpo

# ======================================================================
# 4. Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ - Main Execution
# ======================================================================

def run_all_examples():
    """ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""

    print("\n" + "="*70)
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù†Ø¸Ø§Ù… HPO")
    print("="*70)

    # --- 1. Ù…Ø«Ø§Ù„ Ù…Ø­Ø³Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ© ---
    print("\n\n" + "-"*60)
    print("ğŸ“ˆ 1. Ù…Ø«Ø§Ù„ Ù…Ø­Ø³Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ©")
    print("-"*60)
    # Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø§Ù„ÙŠØ© ÙˆÙ‡Ù…ÙŠØ©
    np.random.seed(42)
    price_data = np.random.randn(252).cumsum() + 100  # 1 Ø³Ù†Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
    returns_data = pd.DataFrame(np.random.randn(252, 10) * 0.01, columns=[f'Asset_{i}' for i in range(10)])

    financial_optimizer = FinancialModelOptimizer(price_data, returns_data, risk_free_rate=0.03)

    # ØªØ­Ø³ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙØ¸Ø©
    financial_optimizer.optimize_portfolio_weights(n_assets=5, n_trials=30)

    # ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ¯Ø§ÙˆÙ„
    financial_optimizer.optimize_trading_strategy(n_trials=25)

    # --- 2. Ù…Ø«Ø§Ù„ Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ---
    print("\n\n" + "-"*60)
    print("ğŸ”¤ 2. Ù…Ø«Ø§Ù„ Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ NLP (Ù…Ø­Ø§ÙƒØ§Ø©)")
    print("-"*60)
    # Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ©
    dummy_texts = ["Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø±Ø§Ø¦Ø¹", "Ø£ÙƒØ±Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ø³ÙŠØ¡", "Ù‡Ø°Ø§ Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹", "Ù„Ø§ Ø¨Ø£Ø³ Ø¨Ù‡"] * 100
    dummy_labels = [1, 0, 1, 0] * 100
    nlp_optimizer = NLPModelOptimizer(dummy_texts, dummy_labels, vocab_size=5000)
    nlp_optimizer.optimize_text_preprocessing(n_trials=15)

    # --- 3. Ù…Ø«Ø§Ù„ Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ---
    print("\n\n" + "-"*60)
    print("ğŸ–¼ï¸ 3. Ù…Ø«Ø§Ù„ Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© (Ù…Ø­Ø§ÙƒØ§Ø©)")
    print("-"*60)
    # Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ© (ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø³ØªÙƒÙˆÙ† ØµÙˆØ±)
    dummy_images = [np.zeros((64, 64, 3)) for _ in range(200)]
    dummy_labels_cv = [0, 1] * 100
    cv_optimizer = ComputerVisionOptimizer(dummy_images, dummy_labels_cv, input_shape=(64, 64, 3))
    cv_optimizer.optimize_data_augmentation(n_trials=15)

    # --- 4. Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ---
    print("\n\n" + "-"*60)
    print("ğŸ¯ 4. Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù (Ù…Ø­Ø§ÙƒØ§Ø©)")
    print("-"*60)
    # Ø£ÙˆØ²Ø§Ù† Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø£Ù‡Ø¯Ø§Ù
    tradeoff_weights = {'accuracy': 0.6, 'speed': 0.25, 'memory': 0.15}
    multi_obj_optimizer = MultiObjectiveOptimizer(objectives_weights=tradeoff_weights)
    multi_obj_optimizer.optimize_model_tradeoffs(n_trials=20)

    # --- 5. Ù…Ø«Ø§Ù„ ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Scikit-learn ---
    if HAS_SKLEARN:
        print("\n\n" + "-"*60)
        print("ğŸ¤– 5. Ù…Ø«Ø§Ù„ ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Scikit-learn (Ø¹Ù…Ù„ÙŠ)")
        print("-"*60)
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Digits Ù„Ù„ØªØ¨Ø³ÙŠØ·
        X, y = load_digits(return_X_y=True)

        sklearn_optimizer = SKLearnOptimizer(X, y, cv_folds=4)

        # ØªØ­Ø³ÙŠÙ† RandomForest
        print("\n--- ØªØ­Ø³ÙŠÙ† RandomForest ---")
        sklearn_optimizer.optimize_random_forest(n_trials=25)

        # ØªØ­Ø³ÙŠÙ† SVC
        print("\n--- ØªØ­Ø³ÙŠÙ† SVC ---")
        sklearn_optimizer.optimize_svc(n_trials=30)
    else:
        print("\n\n" + "-"*60)
        print("ğŸ¤– 5. ØªØ®Ø·ÙŠ Ù…Ø«Ø§Ù„ Scikit-learn (Ø§Ù„Ù…ÙƒØªØ¨Ø© ØºÙŠØ± Ù…Ø«Ø¨ØªØ©)")
        print("-"*60)

    print("\n\n" + "="*70)
    print("ğŸ‰ Ø§ÙƒØªÙ…Ù„ ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­!")
    print("="*70)

if __name__ == '__main__':
    # This script is intended to be run via `run_hpo.py`, which handles the path setup.
    # This block is for direct execution testing.
    try:
        run_all_examples()
    except Exception as e:
        print(f"\nâŒ An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

# ======================================================================
# 3. Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Scikit-learn
# Practical Example: Optimizing Scikit-learn Models
# ======================================================================

class SKLearnOptimizer:
    """Ù…Ø­Ø³Ù† Ù…Ø®ØµØµ Ù„Ù†Ù…Ø§Ø°Ø¬ Scikit-learn"""

    def __init__(self, X, y, cv_folds=5):
        if not HAS_SKLEARN:
            raise ImportError("âŒ ÙŠØªØ·Ù„Ø¨ Scikit-learn. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ«Ø¨ÙŠØª: pip install scikit-learn")

        self.X = X
        self.y = y
        self.cv_folds = cv_folds
        self.scaler = StandardScaler()
        self.X_scaled = self.scaler.fit_transform(self.X)

        print(f"ğŸ¤– Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ Scikit-learn")
        print(f"   Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {self.X.shape}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø·ÙŠØ§Øª (CV): {self.cv_folds}")

    def optimize_random_forest(self, n_trials=40):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª RandomForestClassifier"""

        def rf_objective(trial):
            # ØªØ¹Ø±ÙŠÙ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¨Ø­Ø«
            n_estimators = trial.suggest_int('n_estimators', 50, 500)
            max_depth = trial.suggest_int('max_depth', 5, 50)
            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)
            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                min_samples_split=min_samples_split,
                min_samples_leaf=min_samples_leaf,
                criterion=criterion,
                random_state=42,
                n_jobs=-1
            )

            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹
            cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)
            score = cross_val_score(model, self.X, self.y, cv=cv, scoring='accuracy').mean()

            return score

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        search_space.add_int('n_estimators', 50, 500)
        search_space.add_int('max_depth', 5, 50)
        search_space.add_int('min_samples_split', 2, 20)
        search_space.add_int('min_samples_leaf', 1, 10)
        search_space.add_categorical('criterion', ['gini', 'entropy'])

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=rf_objective,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='sklearn_rf_optimization',
            verbose=True
        )

        best_trial = hpo.optimize()

        if best_trial:
            print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ RandomForest:")
            print(f"   Ø§Ù„Ø¯Ù‚Ø© (CV): {best_trial.value:.4f}")
            print(f"   Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:")
            for param, value in best_trial.params.items():
                print(f"     {param}: {value}")

        return best_trial, hpo

    def optimize_svc(self, n_trials=50):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Support Vector Classifier (SVC)"""

        def svc_objective(trial):
            # ØªØ¹Ø±ÙŠÙ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¨Ø­Ø«
            C = trial.suggest_float('C', 1e-2, 1e2, log=True)
            kernel = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])
            gamma = trial.suggest_float('gamma', 1e-4, 1e-1, log=True)

            # Ø¯Ø±Ø¬Ø© kernel 'poly' ÙÙ‚Ø·
            degree = 3
            if kernel == 'poly':
                degree = trial.suggest_int('degree', 2, 5)

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = SVC(
                C=C,
                kernel=kernel,
                gamma=gamma,
                degree=degree,
                probability=True,
                random_state=42
            )

            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ù‡Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø³Ø© Ù„Ù€ SVC)
            cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)
            score = cross_val_score(model, self.X_scaled, self.y, cv=cv, scoring='accuracy').mean()

            return score

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        search_space.add_uniform('C', 1e-2, 1e2, log=True)
        search_space.add_categorical('kernel', ['rbf', 'poly', 'sigmoid'])
        search_space.add_uniform('gamma', 1e-4, 1e-1, log=True)
        search_space.add_int('degree', 2, 5) # Ø³ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† kernel 'poly'

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=svc_objective,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='sklearn_svc_optimization',
            verbose=True
        )

        best_trial = hpo.optimize()

        if best_trial:
            print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ SVC:")
            print(f"   Ø§Ù„Ø¯Ù‚Ø© (CV): {best_trial.value:.4f}")
            print(f"   Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:")
            for param, value in best_trial.params.items():
                # Ù„Ø§ ØªØ·Ø¨Ø¹ 'degree' Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Kernel Ù‡Ùˆ 'poly'
                if param == 'degree' and best_trial.params.get('kernel') != 'poly':
                    continue
                print(f"     {param}: {value}")

        return best_trial, hpo

class NLPModelOptimizer:
    """Ù…Ø­Ø³Ù† Ù…Ø®ØµØµ Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""

    def __init__(self, text_data, labels, vocab_size=10000):
        self.text_data = text_data
        self.labels = labels
        self.vocab_size = vocab_size

        print(f"ğŸ”¤ Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ NLP")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØµÙˆØµ: {len(text_data)}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {len(set(labels))}")
        print(f"   Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {vocab_size}")

    def optimize_text_preprocessing(self, n_trials=25):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ"""

        def preprocessing_objective(trial):
            # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            min_word_freq = trial.suggest_int('min_word_freq', 1, 10)
            max_features = trial.suggest_int('max_features', 1000, self.vocab_size)
            ngram_range_max = trial.suggest_int('ngram_range_max', 1, 3)
            remove_stopwords = trial.suggest_categorical('remove_stopwords', [True, False])
            lowercase = trial.suggest_categorical('lowercase', [True, False])

            # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
            # (ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ³ØªØ®Ø¯Ù… TfidfVectorizer Ø£Ùˆ Ù…Ø´Ø§Ø¨Ù‡)

            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            base_score = 0.75

            # ØªØ£Ø«ÙŠØ± Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            if min_word_freq > 5:
                base_score += 0.05  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø© ØªØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡

            if max_features > 5000:
                base_score += 0.08  # Ù…ÙØ±Ø¯Ø§Øª Ø£ÙƒØ¨Ø± = ØªÙ…Ø«ÙŠÙ„ Ø£ÙØ¶Ù„
            elif max_features < 2000:
                base_score -= 0.05  # Ù…ÙØ±Ø¯Ø§Øª ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹

            if ngram_range_max > 1:
                base_score += 0.06  # n-grams ØªØ­Ø³Ù† ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚

            if remove_stopwords:
                base_score += 0.03  # Ø¥Ø²Ø§Ù„Ø© stop words Ù…ÙÙŠØ¯Ø© Ø¹Ø§Ø¯Ø©

            if lowercase:
                base_score += 0.02  # Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø© ÙŠÙˆØ­Ø¯ Ø§Ù„Ù†Øµ

            # Ø¶ÙˆØ¶Ø§Ø¡ ÙˆØ§Ù‚Ø¹ÙŠØ©
            noise = np.random.normal(0, 0.05)
            final_score = base_score + noise

            # Ù…Ø­Ø§ÙƒØ§Ø© ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            processing_time = 0.1 + (max_features / 10000) * 0.3
            time.sleep(processing_time)

            return max(0.5, min(0.98, final_score))

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        search_space.add_int('min_word_freq', 1, 10)
        search_space.add_int('max_features', 1000, self.vocab_size)
        search_space.add_int('ngram_range_max', 1, 3)
        search_space.add_categorical('remove_stopwords', [True, False])
        search_space.add_categorical('lowercase', [True, False])

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=preprocessing_objective,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='nlp_preprocessing',
            verbose=True
        )

        best_trial = hpo.optimize()

        if best_trial:
            print(f"\nğŸ¯ Ø£ÙØ¶Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ:")
            print(f"   Ø§Ù„Ø¯Ù‚Ø©: {best_trial.value:.4f}")
            print(f"   ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£Ø¯Ù†Ù‰: {best_trial.params['min_word_freq']}")
            print(f"   Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ù…ÙŠØ²Ø§Øª: {best_trial.params['max_features']:,}")
            print(f"   Ø£Ù‚ØµÙ‰ n-gram: {best_trial.params['ngram_range_max']}")
            print(f"   Ø¥Ø²Ø§Ù„Ø© stop words: {'Ù†Ø¹Ù…' if best_trial.params['remove_stopwords'] else 'Ù„Ø§'}")
            print(f"   Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©: {'Ù†Ø¹Ù…' if best_trial.params['lowercase'] else 'Ù„Ø§'}")

        return best_trial, hpo

class ComputerVisionOptimizer:
    """Ù…Ø­Ø³Ù† Ù…Ø®ØµØµ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©"""

    def __init__(self, image_data, labels, input_shape=(224, 224, 3)):
        self.image_data = image_data
        self.labels = labels
        self.input_shape = input_shape

        print(f"ğŸ–¼ï¸ Ù…Ø­Ø³Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {len(image_data) if hasattr(image_data, '__len__') else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
        print(f"   Ø´ÙƒÙ„ Ø§Ù„Ø¯Ø®Ù„: {input_shape}")

    def optimize_data_augmentation(self, n_trials=20):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Augmentation)"""

        def augmentation_objective(trial):
            # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
            rotation_range = trial.suggest_int('rotation_range', 0, 45)
            zoom_range = trial.suggest_float('zoom_range', 0.0, 0.3)
            horizontal_flip = trial.suggest_categorical('horizontal_flip', [True, False])
            vertical_flip = trial.suggest_categorical('vertical_flip', [True, False])
            brightness_range = trial.suggest_float('brightness_range', 0.0, 0.3)
            contrast_range = trial.suggest_float('contrast_range', 0.0, 0.3)

            # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ£Ø«ÙŠØ± Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡
            base_accuracy = 0.78

            # ØªØ£Ø«ÙŠØ±Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ù…Ø¹Ù‚ÙˆÙ„Ø©
            if 10 <= rotation_range <= 30:
                base_accuracy += 0.08  # Ø¯ÙˆØ±Ø§Ù† Ù…ØªÙˆØ³Ø· Ù…ÙÙŠØ¯
            elif rotation_range > 30:
                base_accuracy += 0.03  # Ø¯ÙˆØ±Ø§Ù† ÙƒØ«ÙŠØ± Ù‚Ø¯ ÙŠØ¶Ø±

            if 0.1 <= zoom_range <= 0.2:
                base_accuracy += 0.06  # ØªÙƒØ¨ÙŠØ± Ù…ØªÙˆØ³Ø· Ù…ÙÙŠØ¯

            if horizontal_flip:
                base_accuracy += 0.05  # Ø§Ù†Ø¹ÙƒØ§Ø³ Ø£ÙÙ‚ÙŠ Ø¹Ø§Ø¯Ø© Ù…ÙÙŠØ¯

            if vertical_flip and rotation_range > 0:
                base_accuracy += 0.02  # Ø§Ù†Ø¹ÙƒØ§Ø³ Ø±Ø£Ø³ÙŠ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ù…ÙÙŠØ¯
            elif vertical_flip:
                base_accuracy -= 0.02  # Ù‚Ø¯ ÙŠØ¶Ø± Ø¨Ø¯ÙˆÙ† Ø¯ÙˆØ±Ø§Ù†

            if 0.1 <= brightness_range <= 0.2:
                base_accuracy += 0.04  # ØªØºÙŠÙŠØ± Ø¥Ø¶Ø§Ø¡Ø© Ù…ØªÙˆØ³Ø·

            if 0.1 <= contrast_range <= 0.2:
                base_accuracy += 0.03  # ØªØºÙŠÙŠØ± ØªØ¨Ø§ÙŠÙ† Ù…ØªÙˆØ³Ø·

            # Ø¹Ù‚ÙˆØ¨Ø© Ù„Ù„Ù…Ø¨Ø§Ù„ØºØ©
            total_augmentation = rotation_range/45 + zoom_range + brightness_range + contrast_range
            if total_augmentation > 1.5:
                base_accuracy -= 0.1  # Ù…Ø¨Ø§Ù„ØºØ© ÙÙŠ Ø§Ù„ØªØ­Ø³ÙŠÙ†

            # Ø¶ÙˆØ¶Ø§Ø¡
            noise = np.random.normal(0, 0.04)
            final_accuracy = base_accuracy + noise

            # Ù…Ø­Ø§ÙƒØ§Ø© ÙˆÙ‚Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ø£ÙƒØ«Ø± ØªØ­Ø³ÙŠÙ† = ÙˆÙ‚Øª Ø£Ø·ÙˆÙ„)
            training_time = 0.5 + total_augmentation * 0.5
            time.sleep(training_time)

            return max(0.6, min(0.95, final_accuracy))

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        search_space.add_int('rotation_range', 0, 45)
        search_space.add_uniform('zoom_range', 0.0, 0.3)
        search_space.add_categorical('horizontal_flip', [True, False])
        search_space.add_categorical('vertical_flip', [True, False])
        search_space.add_uniform('brightness_range', 0.0, 0.3)
        search_space.add_uniform('contrast_range', 0.0, 0.3)

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=augmentation_objective,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='cv_augmentation',
            verbose=True
        )

        best_trial = hpo.optimize()

        if best_trial:
            print(f"\nğŸ¯ Ø£ÙØ¶Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
            print(f"   Ø§Ù„Ø¯Ù‚Ø©: {best_trial.value:.4f}")
            print(f"   Ù…Ø¯Ù‰ Ø§Ù„Ø¯ÙˆØ±Ø§Ù†: {best_trial.params['rotation_range']}Â°")
            print(f"   Ù…Ø¯Ù‰ Ø§Ù„ØªÙƒØ¨ÙŠØ±: {best_trial.params['zoom_range']:.2f}")
            print(f"   Ø§Ù†Ø¹ÙƒØ§Ø³ Ø£ÙÙ‚ÙŠ: {'Ù†Ø¹Ù…' if best_trial.params['horizontal_flip'] else 'Ù„Ø§'}")
            print(f"   Ø§Ù†Ø¹ÙƒØ§Ø³ Ø±Ø£Ø³ÙŠ: {'Ù†Ø¹Ù…' if best_trial.params['vertical_flip'] else 'Ù„Ø§'}")
            print(f"   Ù…Ø¯Ù‰ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©: {best_trial.params['brightness_range']:.2f}")
            print(f"   Ù…Ø¯Ù‰ Ø§Ù„ØªØ¨Ø§ÙŠÙ†: {best_trial.params['contrast_range']:.2f}")

        return best_trial, hpo

# ======================================================================
# 2. Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© - Advanced Features
# ======================================================================

class MultiObjectiveOptimizer:
    """Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù"""

    def __init__(self, objectives_weights=None):
        """
        objectives_weights: Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        Ù…Ø«Ø§Ù„: {'accuracy': 0.7, 'speed': 0.2, 'memory': 0.1}
        """
        self.objectives_weights = objectives_weights or {'primary': 1.0}

        print("ğŸ¯ Ù…Ø­Ø³Ù† Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù")
        print("   Ø§Ù„Ø£Ù‡Ø¯Ø§Ù:")
        for objective, weight in self.objectives_weights.items():
            print(f"     {objective}: {weight:.1%}")

    def optimize_model_tradeoffs(self, n_trials=30):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""

        def multi_objective_function(trial):
            # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model_complexity = trial.suggest_int('model_complexity', 1, 10)
            batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])
            learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)
            regularization = trial.suggest_float('regularization', 0.0, 0.1)

            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            # 1. Ø§Ù„Ø¯Ù‚Ø© (Ø£Ø¹Ù„Ù‰ = Ø£ÙØ¶Ù„)
            base_accuracy = 0.7 + (model_complexity / 10) * 0.2
            if 0.001 <= learning_rate <= 0.01:
                base_accuracy += 0.08
            if 0.01 <= regularization <= 0.05:
                base_accuracy += 0.05
            accuracy = min(0.98, base_accuracy + np.random.normal(0, 0.03))

            # 2. Ø§Ù„Ø³Ø±Ø¹Ø© (ÙˆÙ‚Øª Ø£Ù‚Ù„ = Ø£ÙØ¶Ù„ØŒ Ù†Ø­ÙˆÙ„Ù‡ Ù„Ù†Ù‚Ø§Ø·)
            base_time = 10 + model_complexity * 2 + (batch_size / 32) * 3
            speed_score = max(0.1, 1.0 / base_time)  # Ø¹ÙƒØ³ Ø§Ù„ÙˆÙ‚Øª

            # 3. Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø£Ù‚Ù„ = Ø£ÙØ¶Ù„ØŒ Ù†Ø­ÙˆÙ„Ù‡ Ù„Ù†Ù‚Ø§Ø·)
            memory_usage = model_complexity * 100 + batch_size * 10
            memory_score = max(0.1, 1.0 / (memory_usage / 1000))

            # Ø¯Ù…Ø¬ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
            if 'accuracy' in self.objectives_weights:
                weighted_score = accuracy * self.objectives_weights.get('accuracy', 0)
            else:
                weighted_score = accuracy * 0.6  # ÙˆØ²Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ

            if 'speed' in self.objectives_weights:
                weighted_score += speed_score * self.objectives_weights.get('speed', 0)
            else:
                weighted_score += speed_score * 0.3

            if 'memory' in self.objectives_weights:
                weighted_score += memory_score * self.objectives_weights.get('memory', 0)
            else:
                weighted_score += memory_score * 0.1

            # Ø­ÙØ¸ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†ÙØµÙ„Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„
            trial.user_attrs = {
                'accuracy': accuracy,
                'speed_score': speed_score,
                'memory_score': memory_score,
                'estimated_time': base_time,
                'estimated_memory': memory_usage
            }

            return weighted_score

        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙØ¶Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
        search_space = SearchSpace()
        search_space.add_int('model_complexity', 1, 10)
        search_space.add_categorical('batch_size', [16, 32, 64, 128, 256])
        search_space.add_uniform('learning_rate', 1e-5, 1e-1, log=True)
        search_space.add_uniform('regularization', 0.0, 0.1)

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        hpo = HPOSystem(
            search_space=search_space,
            objective_function=multi_objective_function,
            direction='maximize',
            n_trials=n_trials,
            sampler='TPE',
            study_name='multi_objective_tradeoffs',
            verbose=True
        )

        best_trial_result = hpo.optimize()

        # For detailed printing, access the full trial object from the study, which contains user_attrs
        best_trial_for_printing = hpo.best_trial

        if best_trial_for_printing:
            print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø©:")
            print(f"   Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø©: {best_trial_for_printing.value:.4f}")
            print(f"   Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:")
            print(f"     ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {best_trial_for_printing.params['model_complexity']}")
            print(f"     Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: {best_trial_for_printing.params['batch_size']}")
            print(f"     Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…: {best_trial_for_printing.params['learning_rate']:.5f}")
            print(f"     Ø§Ù„ØªÙ†Ø¸ÙŠÙ…: {best_trial_for_printing.params['regularization']:.4f}")

            print(f"\n   Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ù‚Ø¯Ø±Ø©:")
            if hasattr(best_trial_for_printing, 'user_attrs') and best_trial_for_printing.user_attrs:
                print(f"     Ø§Ù„Ø¯Ù‚Ø©: {best_trial_for_printing.user_attrs.get('accuracy', 0):.4f}")
                print(f"     Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ø±Ø¹Ø©: {best_trial_for_printing.user_attrs.get('speed_score', 0):.4f} (ÙˆÙ‚Øª: {best_trial_for_printing.user_attrs.get('estimated_time', 0):.1f}s)")
                print(f"     Ù†Ù‚Ø§Ø· Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {best_trial_for_printing.user_attrs.get('memory_score', 0):.4f} (Ø°Ø§ÙƒØ±Ø©: {best_trial_for_printing.user_attrs.get('estimated_memory', 0):.1f} MB)")

        return best_trial_result, hpo